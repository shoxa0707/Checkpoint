{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d9ca8a1-fd07-409d-beb7-3e77b78a55e8",
   "metadata": {},
   "source": [
    "# Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb1fa581-8d6f-4099-adc5-7cc416d0757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchtools import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "647901ff-12fa-4d59-9a04-3f50f3b884ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_kwargs = {'batch_size': 32}\n",
    "test_kwargs = {'batch_size': 16}\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    cuda_kwargs = {'num_workers': 1,\n",
    "                   'pin_memory': True,\n",
    "                   'shuffle': True}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a78cdfd7-7b3f-478b-8311-6479daf29700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset1 = datasets.CIFAR100('datasets/data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor())\n",
    "dataset2 = datasets.CIFAR100('datasets/data', train=False,\n",
    "                   transform=transforms.ToTensor())\n",
    "train_data = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "test_data = torch.utils.data.DataLoader(dataset2, **test_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1366f832-6c12-4443-97ee-5af336f11dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our model class\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 256, 3, padding='same')\n",
    "        self.bn1 = nn.BatchNorm2d(256)\n",
    "        self.conv2 = nn.Conv2d(256, 256, 3, padding='same')\n",
    "        self.bn2 = nn.BatchNorm2d(256)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(256, 512, 3, padding='same')\n",
    "        self.bn3 = nn.BatchNorm2d(512)\n",
    "        self.conv4 = nn.Conv2d(512, 512, 3, padding='same')\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(512, 512, 3, padding='same')\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        self.conv6 = nn.Conv2d(512, 512, 3, padding='same')\n",
    "        self.bn6 = nn.BatchNorm2d(512)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "\n",
    "        self.conv7 = nn.Conv2d(512, 512, 3, padding='same')\n",
    "        self.bn7 = nn.BatchNorm2d(512)\n",
    "        self.conv8 = nn.Conv2d(512, 512, 3, padding='same')\n",
    "        self.bn8 = nn.BatchNorm2d(512)\n",
    "        self.dropout4 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        self.dropout5 = nn.Dropout(0.2)\n",
    "        self.bn9 = nn.BatchNorm1d(1024)\n",
    "        self.fc2 = nn.Linear(1024, 100)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.bn3(x))\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.bn4(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = F.relu(self.bn5(x))\n",
    "        x = self.conv6(x)\n",
    "        x = F.relu(self.bn6(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = self.conv7(x)\n",
    "        x = F.relu(self.bn7(x))\n",
    "        x = self.conv8(x)\n",
    "        x = F.relu(self.bn8(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout5(x)\n",
    "        x = self.bn9(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "# train function. get from pytorch github\n",
    "def train(train_data, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_data):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('Train Epoch: {} [{}/{}]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * 32, len(train_data.dataset),\n",
    "            loss.item()), end='\\r')\n",
    "\n",
    "# test function. get from pytorch github\n",
    "def test(test_data):\n",
    "    global test_loss\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_data:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_data.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Correct: {}/{}, Accuracy: {:.0f}%\\n'.format(\n",
    "        test_loss, correct, len(test_data.dataset),\n",
    "        100. * correct / len(test_data.dataset)))\n",
    "\n",
    "# early stopping function\n",
    "def early_stopping(val_loss, args):\n",
    "    score = -val_loss\n",
    "    if args['best_score'] is None:\n",
    "        args['best_score'] = score\n",
    "    elif args['best_score'] + args['delta'] <= score <= args['best_score'] + args['delta']:\n",
    "        args['counter'] += 1\n",
    "        if args['counter'] >= args['patience']:\n",
    "            args['early_stop'] = True\n",
    "    else:\n",
    "        args['best_score'] = score\n",
    "        args['counter'] = 0\n",
    "\n",
    "def save_checkpoint(save_path, args):\n",
    "    # save model to checkpoint\n",
    "    torch.save({\n",
    "        'model': model.state_dict(), \n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'best_score': args['best_score'],\n",
    "        'delta': args['delta']\n",
    "            }, save_path)\n",
    "    print(f\"info: trained model saved {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaef4b7e-085f-4bab-9af3-1490914780c2",
   "metadata": {},
   "source": [
    "### Training initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3f4ca505-a7cf-4238-8d76-440e6c992c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [49984/50000]\tLoss: 3.592701\n",
      "Test set: Average loss: 3.5153, Correct: 1640/10000, Accuracy: 16%\n",
      "\n",
      "Train Epoch: 2 [49984/50000]\tLoss: 3.486880\n",
      "Test set: Average loss: 3.0785, Correct: 2452/10000, Accuracy: 25%\n",
      "\n",
      "Train Epoch: 3 [49984/50000]\tLoss: 2.210239\n",
      "Test set: Average loss: 2.7014, Correct: 3089/10000, Accuracy: 31%\n",
      "\n",
      "Train Epoch: 4 [49984/50000]\tLoss: 3.180290\n",
      "Test set: Average loss: 2.7197, Correct: 2989/10000, Accuracy: 30%\n",
      "\n",
      "Train Epoch: 5 [49984/50000]\tLoss: 2.283243\n",
      "Test set: Average loss: 2.2998, Correct: 3987/10000, Accuracy: 40%\n",
      "\n",
      "Train Epoch: 6 [49984/50000]\tLoss: 2.812876\n",
      "Test set: Average loss: 2.0376, Correct: 4550/10000, Accuracy: 46%\n",
      "\n",
      "Train Epoch: 7 [49984/50000]\tLoss: 1.685385\n",
      "Test set: Average loss: 1.9598, Correct: 4671/10000, Accuracy: 47%\n",
      "\n",
      "Train Epoch: 8 [49984/50000]\tLoss: 1.773578\n",
      "Test set: Average loss: 1.9078, Correct: 4896/10000, Accuracy: 49%\n",
      "\n",
      "Train Epoch: 9 [49984/50000]\tLoss: 2.942346\n",
      "Test set: Average loss: 1.7703, Correct: 5167/10000, Accuracy: 52%\n",
      "\n",
      "Train Epoch: 10 [49984/50000]\tLoss: 1.354552\n",
      "Test set: Average loss: 1.7000, Correct: 5382/10000, Accuracy: 54%\n",
      "\n",
      "info: trained model saved models/cifar100.pt\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = Net()\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# train model\n",
    "epochs = 10\n",
    "\n",
    "# early stop parameters\n",
    "early = {\n",
    "'best_score': None,   # best low loss value\n",
    "'patience': 5,        # maximum number of similarities\n",
    "'delta': 0.001,       # minimum difference between losses\n",
    "'counter': 0,         # early stop counter\n",
    "'early_stop': False   # should early stopping process\n",
    "}\n",
    "save_path = 'models/cifar100.pt' # model save path\n",
    "\n",
    "global test_loss\n",
    "for epoch in range(1, epochs + 1):\n",
    "    test_loss = 0\n",
    "    train(train_data, epoch)\n",
    "    test(test_data)\n",
    "    early_stopping(val_loss, early)\n",
    "    if early['early_stop']:\n",
    "        print(\"Early Stopping: the model loss remained almost unchanged. Stop training process and saving model...\")\n",
    "        save_checkpoint(save_path, early)\n",
    "        break\n",
    "if not early['early_stop']:\n",
    "    save_checkpoint(save_path, early)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a8f3a8-752e-4fc6-8cc5-25231887ba73",
   "metadata": {},
   "source": [
    "### Training checkpoint model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5ca32711-86e7-477d-9240-7bbaea1292d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [49984/50000]\tLoss: 2.571144\n",
      "Test set: Average loss: 2.4259, Correct: 3899/10000, Accuracy: 39%\n",
      "\n",
      "Train Epoch: 2 [49984/50000]\tLoss: 1.016148\n",
      "Test set: Average loss: 1.6442, Correct: 5611/10000, Accuracy: 56%\n",
      "\n",
      "Train Epoch: 3 [49984/50000]\tLoss: 1.354643\n",
      "Test set: Average loss: 1.8350, Correct: 5479/10000, Accuracy: 55%\n",
      "\n",
      "Train Epoch: 4 [49984/50000]\tLoss: 1.263191\n",
      "Test set: Average loss: 1.5290, Correct: 5805/10000, Accuracy: 58%\n",
      "\n",
      "Train Epoch: 5 [49984/50000]\tLoss: 1.456159\n",
      "Test set: Average loss: 1.5875, Correct: 5820/10000, Accuracy: 58%\n",
      "\n",
      "info: trained model saved models/new_cifar100.pt\n"
     ]
    }
   ],
   "source": [
    "# load model from checkpoint\n",
    "model = Net()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "checkpoint = torch.load('models/cifar100.pt')\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "model.to(device)\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "early = {\n",
    "    'best_score': checkpoint['best_score'],\n",
    "    'patience': 5,\n",
    "    'delta': checkpoint['delta'],\n",
    "    'counter': 0,\n",
    "    'early_stop': False\n",
    "}\n",
    "save_path = 'models/new_cifar100.pt'\n",
    "\n",
    "# again train model epoch\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    val_loss = 0\n",
    "    train(train_data, epoch)\n",
    "    test(test_data)\n",
    "    early_stopping(test_loss, early)\n",
    "    if early['early_stop']:\n",
    "        print(\"Early Stopping: the model loss remained almost unchanged. Stop training process and saving model...\")\n",
    "        save_checkpoint(save_path, early)\n",
    "        break\n",
    "if not early['early_stop']:\n",
    "    save_checkpoint(save_path, early)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f1a697-9ff0-4600-ac81-07b95d5b67d7",
   "metadata": {},
   "source": [
    "# Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a138bcec-1bee-4f08-b083-db3d962f938f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32c37f44-5e6b-46ce-ac4e-3b72f53f84d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar100.load_data()\n",
    "# convert labels to categorical\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ef131b-db76-4eee-962f-964ebd4646aa",
   "metadata": {},
   "source": [
    "### Training initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e60ac929-600f-4863-a882-3e3cbb4b695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.initializers import RandomNormal, Constant\n",
    "\n",
    "def create_model():\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    model.add(keras.layers.Rescaling(1/255.0, input_shape=(32,32,3)))\n",
    "    model.add(keras.layers.Conv2D(256,(3,3),padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.Conv2D(256,(3,3),padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "     \n",
    "    model.add(keras.layers.Conv2D(512,(3,3),padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.Conv2D(512,(3,3),padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "    \n",
    "    model.add(keras.layers.Conv2D(512,(3,3),padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.Conv2D(512,(3,3),padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "    \n",
    "    model.add(keras.layers.Conv2D(512,(3,3),padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.Conv2D(512,(3,3),padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "    \n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(1024))\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.BatchNormalization(momentum=0.95, \n",
    "            epsilon=0.005,\n",
    "            beta_initializer=RandomNormal(mean=0.0, stddev=0.05), \n",
    "            gamma_initializer=Constant(value=0.9)))\n",
    "    model.add(keras.layers.Dense(100,activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d2e292e-6d5d-4d58-a2bb-50744573a37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path=\"models/cifar100.hdf5\"\n",
    "checkpoint = ModelCheckpoint(save_path, monitor='val_acc', verbose=1, save_best_only=True, mode='min')\n",
    "es = EarlyStopping(monitor='val_acc', patience=5)\n",
    "callbacks_list = [checkpoint, es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9df9308-6c58-4e33-b9de-ff507ac325ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 4.5214 - acc: 0.0353\n",
      "Epoch 1: val_acc improved from inf to 0.02006, saving model to models\\cifar100.hdf5\n",
      "100/100 [==============================] - 33s 219ms/step - loss: 4.5214 - acc: 0.0353 - val_loss: 4.5678 - val_acc: 0.0201\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 4.1966 - acc: 0.0634\n",
      "Epoch 2: val_acc improved from 0.02006 to 0.01794, saving model to models\\cifar100.hdf5\n",
      "100/100 [==============================] - 21s 208ms/step - loss: 4.1966 - acc: 0.0634 - val_loss: 4.8135 - val_acc: 0.0179\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 4.0468 - acc: 0.0783\n",
      "Epoch 3: val_acc did not improve from 0.01794\n",
      "100/100 [==============================] - 21s 206ms/step - loss: 4.0468 - acc: 0.0783 - val_loss: 5.2900 - val_acc: 0.0200\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.8946 - acc: 0.1013\n",
      "Epoch 4: val_acc did not improve from 0.01794\n",
      "100/100 [==============================] - 21s 206ms/step - loss: 3.8946 - acc: 0.1013 - val_loss: 4.7740 - val_acc: 0.0348\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.7313 - acc: 0.1245\n",
      "Epoch 5: val_acc did not improve from 0.01794\n",
      "100/100 [==============================] - 21s 209ms/step - loss: 3.7313 - acc: 0.1245 - val_loss: 4.2969 - val_acc: 0.0541\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.5437 - acc: 0.1510\n",
      "Epoch 6: val_acc did not improve from 0.01794\n",
      "100/100 [==============================] - 22s 222ms/step - loss: 3.5437 - acc: 0.1510 - val_loss: 3.9638 - val_acc: 0.0959\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.4559 - acc: 0.1637\n",
      "Epoch 7: val_acc did not improve from 0.01794\n",
      "100/100 [==============================] - 21s 207ms/step - loss: 3.4559 - acc: 0.1637 - val_loss: 3.6429 - val_acc: 0.1338\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.3119 - acc: 0.1941\n",
      "Epoch 8: val_acc did not improve from 0.01794\n",
      "100/100 [==============================] - 21s 209ms/step - loss: 3.3119 - acc: 0.1941 - val_loss: 4.2404 - val_acc: 0.1062\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.2174 - acc: 0.2077\n",
      "Epoch 9: val_acc did not improve from 0.01794\n",
      "100/100 [==============================] - 21s 209ms/step - loss: 3.2174 - acc: 0.2077 - val_loss: 3.6124 - val_acc: 0.1751\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.1210 - acc: 0.2295\n",
      "Epoch 10: val_acc did not improve from 0.01794\n",
      "100/100 [==============================] - 21s 209ms/step - loss: 3.1210 - acc: 0.2295 - val_loss: 3.5552 - val_acc: 0.1707\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16081231220>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model()\n",
    "\n",
    "model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks_list,\n",
    "    steps_per_epoch=100, \n",
    "    epochs=10,\n",
    "    validation_split=0.33,\n",
    "    verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df981ffe-7c29-4d93-99e8-9d71523f9b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 8s 23ms/step - loss: 3.5174 - acc: 0.1802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.5173535346984863, 0.18019999563694]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7364cb2e-40d2-4185-8651-68f004157284",
   "metadata": {},
   "source": [
    "### Training checkpoint model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52856549-8ac0-447f-b5e4-13102e9081d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "100/100 [==============================] - 22s 210ms/step - loss: 4.2181 - acc: 0.0616 - val_loss: 4.7459 - val_acc: 0.0392\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 21s 208ms/step - loss: 4.0140 - acc: 0.0908 - val_loss: 5.1755 - val_acc: 0.0539\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 21s 208ms/step - loss: 3.9054 - acc: 0.0988 - val_loss: 3.9354 - val_acc: 0.0959\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 21s 208ms/step - loss: 3.7132 - acc: 0.1278 - val_loss: 3.7649 - val_acc: 0.1110\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 21s 209ms/step - loss: 3.5865 - acc: 0.1448 - val_loss: 3.7800 - val_acc: 0.1236\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1620d105e50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.load_weights('models/cifar100.hdf5')\n",
    "\n",
    "model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=64,\n",
    "    steps_per_epoch=100, \n",
    "    epochs=5,\n",
    "    validation_split=0.33,\n",
    "    verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6f46894-2dbb-473e-9b54-c6af57002d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 7s 22ms/step - loss: 3.7801 - acc: 0.1229\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.7801249027252197, 0.12290000170469284]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc546c36-34b0-4f79-8fe1-c827a3754d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"models/new_cifar100.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9ff9c1-9db9-4626-a300-e39563d6158b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
